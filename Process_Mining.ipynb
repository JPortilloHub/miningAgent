{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhUtxR1_UpTI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ProcessMiningTool:\n",
        "    \"\"\"\n",
        "    A comprehensive process mining tool for analyzing AI agent execution logs.\n",
        "    Produces workflow visualizations with timing metrics and KPI statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv_path):\n",
        "        \"\"\"Initialize the tool and load data.\"\"\"\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.preprocess_data()\n",
        "        self.transitions = None\n",
        "        self.kpis = None\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"Preprocess timestamps and sort data.\"\"\"\n",
        "        # Normalize column names to lowercase for consistency\n",
        "        self.df.columns = self.df.columns.str.lower().str.strip()\n",
        "\n",
        "        # Convert timestamp to datetime\n",
        "        self.df['timestamp'] = pd.to_datetime(self.df['timestamp'])\n",
        "\n",
        "        # Sort by caseid and timestamp\n",
        "        self.df = self.df.sort_values(['caseid', 'timestamp']).reset_index(drop=True)\n",
        "\n",
        "        # Display data overview\n",
        "        print(f\"‚úì Loaded {len(self.df)} events from {self.df['caseid'].nunique()} cases\")\n",
        "        print(f\"‚úì Date range: {self.df['timestamp'].min()} to {self.df['timestamp'].max()}\")\n",
        "        print(f\"‚úì Unique actions: {self.df['action'].nunique()}\")\n",
        "        print(f\"\\nüìã Action distribution:\")\n",
        "        for action, count in self.df['action'].value_counts().items():\n",
        "            print(f\"   ‚Ä¢ {action}: {count}\")\n",
        "\n",
        "    def compute_kpis(self):\n",
        "        \"\"\"Compute case-level KPIs.\"\"\"\n",
        "        case_durations = []\n",
        "\n",
        "        for caseid, group in self.df.groupby('caseid'):\n",
        "            min_time = group['timestamp'].min()\n",
        "            max_time = group['timestamp'].max()\n",
        "            duration = (max_time - min_time).total_seconds()\n",
        "            case_durations.append(duration)\n",
        "\n",
        "        self.kpis = {\n",
        "            'total_cases': len(case_durations),\n",
        "            'avg_duration_seconds': np.mean(case_durations),\n",
        "            'median_duration_seconds': np.median(case_durations),\n",
        "            'min_duration_seconds': np.min(case_durations),\n",
        "            'max_duration_seconds': np.max(case_durations)\n",
        "        }\n",
        "\n",
        "        return self.kpis\n",
        "\n",
        "    def extract_transitions(self):\n",
        "        \"\"\"Extract all transitions between actions with timing information.\"\"\"\n",
        "        transitions = []\n",
        "\n",
        "        for caseid, group in self.df.groupby('caseid'):\n",
        "            actions = group['action'].tolist()\n",
        "            timestamps = group['timestamp'].tolist()\n",
        "\n",
        "            # Only process cases with at least 2 actions\n",
        "            if len(actions) < 2:\n",
        "                print(f\"‚ö†Ô∏è  Warning: Case {caseid} has only {len(actions)} action(s), skipping transitions\")\n",
        "                continue\n",
        "\n",
        "            for i in range(len(actions) - 1):\n",
        "                source = actions[i]\n",
        "                target = actions[i + 1]\n",
        "                duration = (timestamps[i + 1] - timestamps[i]).total_seconds()\n",
        "\n",
        "                transitions.append({\n",
        "                    'source': source,\n",
        "                    'target': target,\n",
        "                    'duration': duration,\n",
        "                    'caseid': caseid\n",
        "                })\n",
        "\n",
        "        self.transitions = pd.DataFrame(transitions)\n",
        "\n",
        "        if len(self.transitions) == 0:\n",
        "            print(\"‚ö†Ô∏è  No transitions found in the data!\")\n",
        "        else:\n",
        "            print(f\"‚úì Extracted {len(self.transitions)} transitions\")\n",
        "\n",
        "        return self.transitions\n",
        "\n",
        "    def compute_transition_metrics(self):\n",
        "        \"\"\"Aggregate transition metrics (average duration and count).\"\"\"\n",
        "        if self.transitions is None:\n",
        "            self.extract_transitions()\n",
        "\n",
        "        metrics = self.transitions.groupby(['source', 'target']).agg({\n",
        "            'duration': ['mean', 'count']\n",
        "        }).reset_index()\n",
        "\n",
        "        metrics.columns = ['source', 'target', 'avg_duration', 'count']\n",
        "        return metrics\n",
        "\n",
        "    @staticmethod\n",
        "    def format_duration(seconds):\n",
        "        \"\"\"Format duration in human-readable form.\"\"\"\n",
        "        if seconds < 1:\n",
        "            return f\"{seconds*1000:.0f}ms\"\n",
        "        elif seconds < 60:\n",
        "            return f\"{seconds:.1f}s\"\n",
        "        elif seconds < 3600:\n",
        "            return f\"{seconds/60:.1f}m\"\n",
        "        elif seconds < 86400:\n",
        "            return f\"{seconds/3600:.1f}h\"\n",
        "        else:\n",
        "            return f\"{seconds/86400:.1f}d\"\n",
        "\n",
        "    def build_process_graph(self):\n",
        "        \"\"\"Build a directed graph representing the process flow.\"\"\"\n",
        "        metrics = self.compute_transition_metrics()\n",
        "\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        # Add all unique actions as nodes\n",
        "        all_actions = set(metrics['source'].unique()) | set(metrics['target'].unique())\n",
        "        G.add_nodes_from(all_actions)\n",
        "\n",
        "        # Add edges with attributes\n",
        "        for _, row in metrics.iterrows():\n",
        "            avg_dur_formatted = self.format_duration(row['avg_duration'])\n",
        "            edge_label = f\"{avg_dur_formatted} / {int(row['count'])}\"\n",
        "\n",
        "            G.add_edge(\n",
        "                row['source'],\n",
        "                row['target'],\n",
        "                weight=row['count'],\n",
        "                avg_duration=row['avg_duration'],\n",
        "                label=edge_label\n",
        "            )\n",
        "\n",
        "        return G\n",
        "\n",
        "    def visualize_workflow(self, figsize=(16, 12), output_path=None):\n",
        "        \"\"\"Create a workflow visualization with timing and frequency annotations.\"\"\"\n",
        "        G = self.build_process_graph()\n",
        "\n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "        # Use hierarchical layout for better readability\n",
        "        try:\n",
        "            pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
        "        except:\n",
        "            pos = nx.shell_layout(G)\n",
        "\n",
        "        # Calculate node sizes based on frequency (total incoming + outgoing)\n",
        "        node_frequencies = defaultdict(int)\n",
        "        for edge in G.edges():\n",
        "            node_frequencies[edge[0]] += G.edges[edge]['weight']\n",
        "            node_frequencies[edge[1]] += G.edges[edge]['weight']\n",
        "\n",
        "        max_freq = max(node_frequencies.values()) if node_frequencies else 1\n",
        "        node_sizes = [3000 + (node_frequencies[node] / max_freq) * 5000 for node in G.nodes()]\n",
        "\n",
        "        # Draw nodes\n",
        "        nx.draw_networkx_nodes(\n",
        "            G, pos,\n",
        "            node_size=node_sizes,\n",
        "            node_color='lightblue',\n",
        "            edgecolors='darkblue',\n",
        "            linewidths=2,\n",
        "            ax=ax\n",
        "        )\n",
        "\n",
        "        # Draw edges with varying thickness based on frequency\n",
        "        edge_weights = [G.edges[edge]['weight'] for edge in G.edges()]\n",
        "        max_weight = max(edge_weights) if edge_weights else 1\n",
        "        edge_widths = [1 + (w / max_weight) * 4 for w in edge_weights]\n",
        "\n",
        "        nx.draw_networkx_edges(\n",
        "            G, pos,\n",
        "            width=edge_widths,\n",
        "            edge_color='gray',\n",
        "            arrows=True,\n",
        "            arrowsize=20,\n",
        "            arrowstyle='->',\n",
        "            connectionstyle='arc3,rad=0.1',\n",
        "            ax=ax\n",
        "        )\n",
        "\n",
        "        # Draw node labels\n",
        "        nx.draw_networkx_labels(\n",
        "            G, pos,\n",
        "            font_size=10,\n",
        "            font_weight='bold',\n",
        "            font_color='darkblue',\n",
        "            ax=ax\n",
        "        )\n",
        "\n",
        "        # Draw edge labels\n",
        "        edge_labels = nx.get_edge_attributes(G, 'label')\n",
        "        nx.draw_networkx_edge_labels(\n",
        "            G, pos,\n",
        "            edge_labels=edge_labels,\n",
        "            font_size=8,\n",
        "            font_color='red',\n",
        "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='none', alpha=0.7),\n",
        "            ax=ax\n",
        "        )\n",
        "\n",
        "        ax.set_title('AI Agent Process Flow\\n(Edge labels: avg_duration / occurrence_count)',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "        ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if output_path:\n",
        "            plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"‚úì Workflow visualization saved to {output_path}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        return G\n",
        "\n",
        "    def print_kpi_report(self):\n",
        "        \"\"\"Print a formatted KPI report.\"\"\"\n",
        "        if self.kpis is None:\n",
        "            self.compute_kpis()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìä PROCESS MINING KPI REPORT\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"\\nüìÅ Total Cases: {self.kpis['total_cases']}\")\n",
        "        print(f\"\\n‚è±Ô∏è  Case Duration Statistics:\")\n",
        "        print(f\"   ‚Ä¢ Average:  {self.format_duration(self.kpis['avg_duration_seconds'])}\")\n",
        "        print(f\"   ‚Ä¢ Median:   {self.format_duration(self.kpis['median_duration_seconds'])}\")\n",
        "        print(f\"   ‚Ä¢ Minimum:  {self.format_duration(self.kpis['min_duration_seconds'])}\")\n",
        "        print(f\"   ‚Ä¢ Maximum:  {self.format_duration(self.kpis['max_duration_seconds'])}\")\n",
        "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "    def print_transition_summary(self, top_n=10):\n",
        "        \"\"\"Print summary of most frequent transitions.\"\"\"\n",
        "        metrics = self.compute_transition_metrics()\n",
        "        metrics_sorted = metrics.sort_values('count', ascending=False)\n",
        "\n",
        "        print(f\"\\nüîÑ Top {top_n} Most Frequent Transitions:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{'Source':<25} {'Target':<25} {'Avg Duration':<15} {'Count':<10}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        for _, row in metrics_sorted.head(top_n).iterrows():\n",
        "            duration_str = self.format_duration(row['avg_duration'])\n",
        "            print(f\"{row['source']:<25} {row['target']:<25} {duration_str:<15} {int(row['count']):<10}\")\n",
        "\n",
        "        print(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "    def run_complete_analysis(self, output_path='process_workflow.png'):\n",
        "        \"\"\"Run the complete process mining analysis pipeline.\"\"\"\n",
        "        print(\"\\nüöÄ Starting Process Mining Analysis...\\n\")\n",
        "\n",
        "        # Compute KPIs\n",
        "        self.compute_kpis()\n",
        "        self.print_kpi_report()\n",
        "\n",
        "        # Extract and summarize transitions\n",
        "        self.extract_transitions()\n",
        "        self.print_transition_summary()\n",
        "\n",
        "        # Visualize workflow\n",
        "        print(\"üìà Generating workflow visualization...\\n\")\n",
        "        self.visualize_workflow(output_path=output_path)\n",
        "\n",
        "        print(\"‚úÖ Analysis complete!\")\n",
        "\n",
        "        return self"
      ],
      "metadata": {
        "id": "l-g_7jzjUsyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# USAGE EXAMPLE\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the tool with your CSV file\n",
        "    # Replace 'agent_logs.csv' with your actual CSV filename\n",
        "    csv_filename = 'Sample.csv'  # Change this to your file name\n",
        "\n",
        "    try:\n",
        "        tool = ProcessMiningTool(csv_filename)\n",
        "\n",
        "        # Run complete analysis\n",
        "        tool.run_complete_analysis(output_path='agent_workflow.png')\n",
        "\n",
        "        # Optional: Access individual components\n",
        "        # kpis = tool.compute_kpis()\n",
        "        # transitions = tool.extract_transitions()\n",
        "        # graph = tool.build_process_graph()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: File '{csv_filename}' not found!\")\n",
        "        print(\"Please ensure the CSV file is in the same directory as this script.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error occurred: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ],
      "metadata": {
        "id": "kF6WofzmU3k1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}