{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhUtxR1_UpTI"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/JPortilloHub/miningAgent.git\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import networkx as nx\n",
        "import re\n",
        "import csv\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ProcessMiningTool:\n",
        "    \"\"\"\n",
        "    A comprehensive process mining tool for analyzing AI agent execution logs.\n",
        "    Produces workflow visualizations with timing metrics and KPI statistics.\n",
        "    Includes variant analysis, bottleneck detection, and interactive visualizations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        Initialize the tool and load data.\n",
        "\n",
        "        Args:\n",
        "            data: Either a pandas DataFrame, a list of dictionaries (from csv.DictReader),\n",
        "                  or a string path to a CSV file.\n",
        "        \"\"\"\n",
        "        if isinstance(data, pd.DataFrame):\n",
        "            self.df = data.copy()\n",
        "        elif isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):\n",
        "            # Convert list of dictionaries to DataFrame\n",
        "            self.df = pd.DataFrame(data)\n",
        "        elif isinstance(data, str):\n",
        "            # Assume it's a file path\n",
        "            self.df = pd.read_csv(data)\n",
        "        else:\n",
        "            raise ValueError(\"Input must be a DataFrame, list of dictionaries, or CSV file path\")\n",
        "\n",
        "        self.preprocess_data()\n",
        "        self.transitions = None\n",
        "        self.kpis = None\n",
        "        self.variants = None\n",
        "        self.bottlenecks = None\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"Preprocess timestamps and sort data.\"\"\"\n",
        "        # Normalize column names to lowercase for consistency\n",
        "        self.df.columns = self.df.columns.str.lower().str.strip()\n",
        "\n",
        "        # Convert timestamp to datetime\n",
        "        self.df['timestamp'] = pd.to_datetime(self.df['timestamp'])\n",
        "\n",
        "        # Sort by caseid and timestamp\n",
        "        self.df = self.df.sort_values(['caseid', 'timestamp']).reset_index(drop=True)\n",
        "\n",
        "        # Display data overview\n",
        "        print(f\"‚úì Loaded {len(self.df)} events from {self.df['caseid'].nunique()} cases\")\n",
        "        print(f\"‚úì Date range: {self.df['timestamp'].min()} to {self.df['timestamp'].max()}\")\n",
        "        print(f\"‚úì Unique actions: {self.df['action'].nunique()}\")\n",
        "        print(f\"\\nüìã Action distribution:\")\n",
        "        for action, count in self.df['action'].value_counts().items():\n",
        "            print(f\"   ‚Ä¢ {action}: {count}\")\n",
        "\n",
        "    def compute_kpis(self):\n",
        "        \"\"\"Compute case-level KPIs.\"\"\"\n",
        "        case_durations = []\n",
        "        case_activity_counts = []\n",
        "\n",
        "        for caseid, group in self.df.groupby('caseid'):\n",
        "            min_time = group['timestamp'].min()\n",
        "            max_time = group['timestamp'].max()\n",
        "            duration = (max_time - min_time).total_seconds()\n",
        "            case_durations.append(duration)\n",
        "            case_activity_counts.append(len(group))\n",
        "\n",
        "        self.kpis = {\n",
        "            'total_cases': len(case_durations),\n",
        "            'total_events': len(self.df),\n",
        "            'avg_duration_seconds': np.mean(case_durations),\n",
        "            'median_duration_seconds': np.median(case_durations),\n",
        "            'min_duration_seconds': np.min(case_durations),\n",
        "            'max_duration_seconds': np.max(case_durations),\n",
        "            'std_duration_seconds': np.std(case_durations),\n",
        "            'avg_activities_per_case': np.mean(case_activity_counts),\n",
        "            'median_activities_per_case': np.median(case_activity_counts)\n",
        "        }\n",
        "\n",
        "        return self.kpis\n",
        "\n",
        "    def extract_transitions(self):\n",
        "        \"\"\"Extract all transitions between actions with timing information.\"\"\"\n",
        "        transitions = []\n",
        "\n",
        "        for caseid, group in self.df.groupby('caseid'):\n",
        "            actions = group['action'].tolist()\n",
        "            timestamps = group['timestamp'].tolist()\n",
        "\n",
        "            # Only process cases with at least 2 actions\n",
        "            if len(actions) < 2:\n",
        "                print(f\"‚ö†Ô∏è  Warning: Case {caseid} has only {len(actions)} action(s), skipping transitions\")\n",
        "                continue\n",
        "\n",
        "            for i in range(len(actions) - 1):\n",
        "                source = actions[i]\n",
        "                target = actions[i + 1]\n",
        "                duration = (timestamps[i + 1] - timestamps[i]).total_seconds()\n",
        "\n",
        "                transitions.append({\n",
        "                    'source': source,\n",
        "                    'target': target,\n",
        "                    'duration': duration,\n",
        "                    'caseid': caseid\n",
        "                })\n",
        "\n",
        "        self.transitions = pd.DataFrame(transitions)\n",
        "\n",
        "        if len(self.transitions) == 0:\n",
        "            print(\"‚ö†Ô∏è  No transitions found in the data!\")\n",
        "        else:\n",
        "            print(f\"‚úì Extracted {len(self.transitions)} transitions\")\n",
        "\n",
        "        return self.transitions\n",
        "\n",
        "    def compute_transition_metrics(self):\n",
        "        \"\"\"Aggregate transition metrics (average duration and count).\"\"\"\n",
        "        if self.transitions is None:\n",
        "            self.extract_transitions()\n",
        "\n",
        "        metrics = self.transitions.groupby(['source', 'target']).agg({\n",
        "            'duration': ['mean', 'median', 'std', 'min', 'max', 'count']\n",
        "        }).reset_index()\n",
        "\n",
        "        metrics.columns = ['source', 'target', 'avg_duration', 'median_duration',\n",
        "                          'std_duration', 'min_duration', 'max_duration', 'count']\n",
        "        return metrics\n",
        "\n",
        "    def analyze_variants(self):\n",
        "        \"\"\"Analyze process variants (unique paths through the process).\"\"\"\n",
        "        variants = []\n",
        "\n",
        "        for caseid, group in self.df.groupby('caseid'):\n",
        "            actions = group['action'].tolist()\n",
        "            variant = ' ‚Üí '.join(actions)\n",
        "\n",
        "            min_time = group['timestamp'].min()\n",
        "            max_time = group['timestamp'].max()\n",
        "            duration = (max_time - min_time).total_seconds()\n",
        "\n",
        "            variants.append({\n",
        "                'caseid': caseid,\n",
        "                'variant': variant,\n",
        "                'duration': duration,\n",
        "                'num_activities': len(actions)\n",
        "            })\n",
        "\n",
        "        self.variants = pd.DataFrame(variants)\n",
        "\n",
        "        # Compute variant statistics\n",
        "        variant_stats = self.variants.groupby('variant').agg({\n",
        "            'caseid': 'count',\n",
        "            'duration': ['mean', 'median', 'std'],\n",
        "            'num_activities': 'first'\n",
        "        }).reset_index()\n",
        "\n",
        "        variant_stats.columns = ['variant', 'frequency', 'avg_duration',\n",
        "                                'median_duration', 'std_duration', 'num_activities']\n",
        "        variant_stats = variant_stats.sort_values('frequency', ascending=False)\n",
        "        variant_stats['percentage'] = (variant_stats['frequency'] / len(self.variants) * 100).round(2)\n",
        "\n",
        "        return variant_stats\n",
        "\n",
        "    def detect_bottlenecks(self, threshold_percentile=75):\n",
        "        \"\"\"\n",
        "        Detect bottlenecks based on transition duration.\n",
        "\n",
        "        Args:\n",
        "            threshold_percentile: Percentile threshold for identifying slow transitions (default 75)\n",
        "        \"\"\"\n",
        "        if self.transitions is None:\n",
        "            self.extract_transitions()\n",
        "\n",
        "        metrics = self.compute_transition_metrics()\n",
        "\n",
        "        # Calculate threshold\n",
        "        duration_threshold = self.transitions['duration'].quantile(threshold_percentile / 100)\n",
        "\n",
        "        # Identify bottlenecks\n",
        "        bottlenecks = metrics[metrics['avg_duration'] > duration_threshold].copy()\n",
        "        bottlenecks = bottlenecks.sort_values('avg_duration', ascending=False)\n",
        "\n",
        "        # Calculate variability (coefficient of variation)\n",
        "        bottlenecks['cv'] = bottlenecks['std_duration'] / bottlenecks['avg_duration']\n",
        "\n",
        "        self.bottlenecks = bottlenecks\n",
        "\n",
        "        return bottlenecks\n",
        "\n",
        "    @staticmethod\n",
        "    def format_duration(seconds):\n",
        "        \"\"\"Format duration in human-readable form.\"\"\"\n",
        "        if seconds < 1:\n",
        "            return f\"{seconds*1000:.0f}ms\"\n",
        "        elif seconds < 60:\n",
        "            return f\"{seconds:.1f}s\"\n",
        "        elif seconds < 3600:\n",
        "            return f\"{seconds/60:.1f}m\"\n",
        "        elif seconds < 86400:\n",
        "            return f\"{seconds/3600:.1f}h\"\n",
        "        else:\n",
        "            return f\"{seconds/86400:.1f}d\"\n",
        "\n",
        "    def build_process_graph(self):\n",
        "        \"\"\"Build a directed graph representing the process flow.\"\"\"\n",
        "        metrics = self.compute_transition_metrics()\n",
        "\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        # Add all unique actions as nodes\n",
        "        all_actions = set(metrics['source'].unique()) | set(metrics['target'].unique())\n",
        "        G.add_nodes_from(all_actions)\n",
        "\n",
        "        # Add edges with attributes\n",
        "        for _, row in metrics.iterrows():\n",
        "            avg_dur_formatted = self.format_duration(row['avg_duration'])\n",
        "            edge_label = f\"{avg_dur_formatted} / {int(row['count'])}\"\n",
        "\n",
        "            G.add_edge(\n",
        "                row['source'],\n",
        "                row['target'],\n",
        "                weight=row['count'],\n",
        "                avg_duration=row['avg_duration'],\n",
        "                median_duration=row['median_duration'],\n",
        "                label=edge_label\n",
        "            )\n",
        "\n",
        "        return G\n",
        "\n",
        "    def visualize_workflow_static(self, figsize=(18, 14), output_path=None):\n",
        "        \"\"\"Create a static workflow visualization with matplotlib using hierarchical layout.\"\"\"\n",
        "        G = self.build_process_graph()\n",
        "\n",
        "        # Identify start and end nodes\n",
        "        all_sources = set(self.df.groupby('caseid')['action'].first())\n",
        "        all_targets = set(self.df.groupby('caseid')['action'].last())\n",
        "\n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "        # Use hierarchical layout (top to bottom)\n",
        "        try:\n",
        "            # Try to get a hierarchical layout\n",
        "            pos = nx.nx_agraph.graphviz_layout(G, prog='dot')\n",
        "        except:\n",
        "            try:\n",
        "                # Fallback to manual hierarchical layout\n",
        "                pos = self._hierarchical_layout(G, all_sources, all_targets)\n",
        "            except:\n",
        "                # Final fallback\n",
        "                pos = nx.spring_layout(G, k=3, iterations=100, seed=42)\n",
        "\n",
        "        # Invert y-axis so start is at top\n",
        "        pos = {node: (x, -y) for node, (x, y) in pos.items()}\n",
        "\n",
        "        # Calculate node sizes based on frequency\n",
        "        node_frequencies = defaultdict(int)\n",
        "        for edge in G.edges():\n",
        "            node_frequencies[edge[0]] += G.edges[edge]['weight']\n",
        "            node_frequencies[edge[1]] += G.edges[edge]['weight']\n",
        "\n",
        "        max_freq = max(node_frequencies.values()) if node_frequencies else 1\n",
        "        node_sizes = [2000 + (node_frequencies[node] / max_freq) * 6000 for node in G.nodes()]\n",
        "\n",
        "        # Color nodes based on type (start, end, intermediate)\n",
        "        node_colors = []\n",
        "        for node in G.nodes():\n",
        "            if node in all_sources and node in all_targets:\n",
        "                node_colors.append('#90EE90')  # Light green for start/end\n",
        "            elif node in all_sources:\n",
        "                node_colors.append('#87CEEB')  # Sky blue for start\n",
        "            elif node in all_targets:\n",
        "                node_colors.append('#FFB6C1')  # Light pink for end\n",
        "            else:\n",
        "                node_colors.append('#E6E6FA')  # Lavender for intermediate\n",
        "\n",
        "        # Draw edges with varying thickness and curved arrows\n",
        "        edge_weights = [G.edges[edge]['weight'] for edge in G.edges()]\n",
        "        max_weight = max(edge_weights) if edge_weights else 1\n",
        "\n",
        "        for edge in G.edges():\n",
        "            weight = G.edges[edge]['weight']\n",
        "            width = 1 + (weight / max_weight) * 5\n",
        "\n",
        "            # Draw edge with proper arrow\n",
        "            ax.annotate('',\n",
        "                       xy=pos[edge[1]], xycoords='data',\n",
        "                       xytext=pos[edge[0]], textcoords='data',\n",
        "                       arrowprops=dict(arrowstyle='-|>',\n",
        "                                     connectionstyle='arc3,rad=0.1',\n",
        "                                     lw=width,\n",
        "                                     color='#4A90E2',\n",
        "                                     alpha=0.6,\n",
        "                                     shrinkA=15, shrinkB=15))\n",
        "\n",
        "        # Draw nodes\n",
        "        nx.draw_networkx_nodes(\n",
        "            G, pos,\n",
        "            node_size=node_sizes,\n",
        "            node_color=node_colors,\n",
        "            edgecolors='#2C3E50',\n",
        "            linewidths=2.5,\n",
        "            ax=ax\n",
        "        )\n",
        "\n",
        "        # Draw node labels with better formatting\n",
        "        labels = {node: node for node in G.nodes()}\n",
        "        nx.draw_networkx_labels(\n",
        "            G, pos,\n",
        "            labels=labels,\n",
        "            font_size=9,\n",
        "            font_weight='bold',\n",
        "            font_color='#2C3E50',\n",
        "            ax=ax\n",
        "        )\n",
        "\n",
        "        # Draw edge labels (duration / count)\n",
        "        edge_labels = nx.get_edge_attributes(G, 'label')\n",
        "        label_pos = {}\n",
        "        for edge in G.edges():\n",
        "            # Calculate label position along the edge\n",
        "            x1, y1 = pos[edge[0]]\n",
        "            x2, y2 = pos[edge[1]]\n",
        "            label_pos[edge] = ((x1 + x2) / 2 + 10, (y1 + y2) / 2)\n",
        "\n",
        "        for edge, label in edge_labels.items():\n",
        "            x, y = label_pos[edge]\n",
        "            ax.text(x, y, label,\n",
        "                   fontsize=8,\n",
        "                   color='#E74C3C',\n",
        "                   fontweight='bold',\n",
        "                   bbox=dict(boxstyle='round,pad=0.4',\n",
        "                           facecolor='white',\n",
        "                           edgecolor='#E74C3C',\n",
        "                           alpha=0.9,\n",
        "                           linewidth=1.5),\n",
        "                   ha='center',\n",
        "                   va='center')\n",
        "\n",
        "        ax.set_title('AI Agent Process Flow\\n(Edge labels: avg_duration / occurrence_count)',\n",
        "                     fontsize=18, fontweight='bold', pad=20, color='#2C3E50')\n",
        "\n",
        "        # Add legend\n",
        "        from matplotlib.patches import Patch\n",
        "        legend_elements = [\n",
        "            Patch(facecolor='#87CEEB', edgecolor='#2C3E50', label='Start Activity'),\n",
        "            Patch(facecolor='#FFB6C1', edgecolor='#2C3E50', label='End Activity'),\n",
        "            Patch(facecolor='#E6E6FA', edgecolor='#2C3E50', label='Intermediate Activity')\n",
        "        ]\n",
        "        ax.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
        "\n",
        "        ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if output_path:\n",
        "            plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "            print(f\"‚úì Static workflow visualization saved to {output_path}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        return G\n",
        "\n",
        "    def _hierarchical_layout(self, G, start_nodes, end_nodes):\n",
        "        \"\"\"Create a hierarchical layout with start at top and end at bottom.\"\"\"\n",
        "        pos = {}\n",
        "\n",
        "        # Perform topological sort to get layers\n",
        "        try:\n",
        "            layers = list(nx.topological_generations(G))\n",
        "        except:\n",
        "            # If not a DAG, use approximation\n",
        "            layers = self._approximate_layers(G, start_nodes, end_nodes)\n",
        "\n",
        "        # Assign positions based on layers\n",
        "        max_layer_width = max(len(layer) for layer in layers)\n",
        "        vertical_spacing = 100\n",
        "        horizontal_spacing = 150\n",
        "\n",
        "        for level, layer in enumerate(layers):\n",
        "            layer_width = len(layer)\n",
        "            start_x = -(layer_width - 1) * horizontal_spacing / 2\n",
        "\n",
        "            for i, node in enumerate(sorted(layer)):\n",
        "                x = start_x + i * horizontal_spacing\n",
        "                y = level * vertical_spacing\n",
        "                pos[node] = (x, y)\n",
        "\n",
        "        return pos\n",
        "\n",
        "    def _approximate_layers(self, G, start_nodes, end_nodes):\n",
        "        \"\"\"Approximate hierarchical layers when graph is not a DAG.\"\"\"\n",
        "        layers = []\n",
        "        visited = set()\n",
        "        current_layer = set(start_nodes)\n",
        "\n",
        "        while current_layer:\n",
        "            layers.append(list(current_layer))\n",
        "            visited.update(current_layer)\n",
        "            next_layer = set()\n",
        "\n",
        "            for node in current_layer:\n",
        "                for successor in G.successors(node):\n",
        "                    if successor not in visited:\n",
        "                        next_layer.add(successor)\n",
        "\n",
        "            current_layer = next_layer\n",
        "\n",
        "            # Prevent infinite loops\n",
        "            if len(layers) > len(G.nodes()):\n",
        "                break\n",
        "\n",
        "        # Add any remaining nodes\n",
        "        remaining = set(G.nodes()) - visited\n",
        "        if remaining:\n",
        "            layers.append(list(remaining))\n",
        "\n",
        "        return layers\n",
        "\n",
        "    def visualize_workflow_interactive(self, output_path='workflow_interactive.html'):\n",
        "        \"\"\"Create an interactive workflow visualization with Plotly using hierarchical layout.\"\"\"\n",
        "        G = self.build_process_graph()\n",
        "        metrics = self.compute_transition_metrics()\n",
        "\n",
        "        # Identify start and end nodes\n",
        "        all_sources = set(self.df.groupby('caseid')['action'].first())\n",
        "        all_targets = set(self.df.groupby('caseid')['action'].last())\n",
        "\n",
        "        # Calculate hierarchical positions\n",
        "        try:\n",
        "            pos = nx.nx_agraph.graphviz_layout(G, prog='dot')\n",
        "        except:\n",
        "            try:\n",
        "                pos = self._hierarchical_layout(G, all_sources, all_targets)\n",
        "            except:\n",
        "                pos = nx.spring_layout(G, k=3, iterations=100, seed=42)\n",
        "\n",
        "        # Invert y-axis (start at top)\n",
        "        pos = {node: (x, -y) for node, (x, y) in pos.items()}\n",
        "\n",
        "        # Calculate node frequencies\n",
        "        node_frequencies = defaultdict(int)\n",
        "        for edge in G.edges():\n",
        "            node_frequencies[edge[0]] += G.edges[edge]['weight']\n",
        "            node_frequencies[edge[1]] += G.edges[edge]['weight']\n",
        "\n",
        "        max_node_freq = max(node_frequencies.values()) if node_frequencies else 1\n",
        "\n",
        "        # Create edge traces with arrows\n",
        "        edge_traces = []\n",
        "        edge_annotations = []\n",
        "\n",
        "        for edge in G.edges():\n",
        "            x0, y0 = pos[edge[0]]\n",
        "            x1, y1 = pos[edge[1]]\n",
        "\n",
        "            weight = G.edges[edge]['weight']\n",
        "            avg_dur = G.edges[edge]['avg_duration']\n",
        "            median_dur = G.edges[edge]['median_duration']\n",
        "\n",
        "            # Edge line\n",
        "            edge_trace = go.Scatter(\n",
        "                x=[x0, x1, None],\n",
        "                y=[y0, y1, None],\n",
        "                mode='lines',\n",
        "                line=dict(\n",
        "                    width=1 + (weight / max([G.edges[e]['weight'] for e in G.edges()]) * 6),\n",
        "                    color='rgba(74, 144, 226, 0.6)'\n",
        "                ),\n",
        "                hoverinfo='text',\n",
        "                text=f\"<b>{edge[0]} ‚Üí {edge[1]}</b><br>\" +\n",
        "                     f\"Frequency: {weight}<br>\" +\n",
        "                     f\"Avg Duration: {self.format_duration(avg_dur)}<br>\" +\n",
        "                     f\"Median Duration: {self.format_duration(median_dur)}\",\n",
        "                showlegend=False\n",
        "            )\n",
        "            edge_traces.append(edge_trace)\n",
        "\n",
        "            # Arrow annotation\n",
        "            edge_annotations.append(\n",
        "                dict(\n",
        "                    ax=x0, ay=y0,\n",
        "                    axref='x', ayref='y',\n",
        "                    x=x1, y=y1,\n",
        "                    xref='x', yref='y',\n",
        "                    showarrow=True,\n",
        "                    arrowhead=2,\n",
        "                    arrowsize=1.5,\n",
        "                    arrowwidth=1 + (weight / max([G.edges[e]['weight'] for e in G.edges()]) * 3),\n",
        "                    arrowcolor='rgba(74, 144, 226, 0.6)',\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Edge label annotation\n",
        "            label_text = f\"{self.format_duration(avg_dur)} / {weight}\"\n",
        "            edge_annotations.append(\n",
        "                dict(\n",
        "                    x=(x0 + x1) / 2,\n",
        "                    y=(y0 + y1) / 2,\n",
        "                    xref='x', yref='y',\n",
        "                    text=label_text,\n",
        "                    showarrow=False,\n",
        "                    font=dict(size=9, color='rgb(231, 76, 60)'),\n",
        "                    bgcolor='rgba(255, 255, 255, 0.9)',\n",
        "                    bordercolor='rgb(231, 76, 60)',\n",
        "                    borderwidth=1,\n",
        "                    borderpad=3\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Create node trace\n",
        "        node_x = []\n",
        "        node_y = []\n",
        "        node_text = []\n",
        "        node_hover = []\n",
        "        node_size = []\n",
        "        node_colors = []\n",
        "\n",
        "        for node in G.nodes():\n",
        "            x, y = pos[node]\n",
        "            node_x.append(x)\n",
        "            node_y.append(y)\n",
        "            node_text.append(node)\n",
        "\n",
        "            freq = node_frequencies[node]\n",
        "            node_size.append(20 + (freq / max_node_freq * 50))\n",
        "\n",
        "            # Color based on node type\n",
        "            if node in all_sources and node in all_targets:\n",
        "                node_colors.append('rgb(144, 238, 144)')  # Light green\n",
        "                node_type = 'Start/End'\n",
        "            elif node in all_sources:\n",
        "                node_colors.append('rgb(135, 206, 235)')  # Sky blue\n",
        "                node_type = 'Start'\n",
        "            elif node in all_targets:\n",
        "                node_colors.append('rgb(255, 182, 193)')  # Light pink\n",
        "                node_type = 'End'\n",
        "            else:\n",
        "                node_colors.append('rgb(230, 230, 250)')  # Lavender\n",
        "                node_type = 'Intermediate'\n",
        "\n",
        "            node_hover.append(f\"<b>{node}</b><br>Type: {node_type}<br>Frequency: {freq}\")\n",
        "\n",
        "        node_trace = go.Scatter(\n",
        "            x=node_x,\n",
        "            y=node_y,\n",
        "            mode='markers+text',\n",
        "            text=node_text,\n",
        "            textposition='middle center',\n",
        "            textfont=dict(size=10, color='rgb(44, 62, 80)', family='Arial Black'),\n",
        "            hoverinfo='text',\n",
        "            hovertext=node_hover,\n",
        "            marker=dict(\n",
        "                size=node_size,\n",
        "                color=node_colors,\n",
        "                line=dict(width=2, color='rgb(44, 62, 80)')\n",
        "            ),\n",
        "            showlegend=False\n",
        "        )\n",
        "\n",
        "        # Create figure\n",
        "        fig = go.Figure(data=edge_traces + [node_trace])\n",
        "\n",
        "        # Add all annotations (arrows and labels)\n",
        "        fig.update_layout(annotations=edge_annotations)\n",
        "\n",
        "        fig.update_layout(\n",
        "            title={\n",
        "                'text': 'Interactive AI Agent Process Flow<br><sub>Hover over nodes and edges for details | Arrows show process direction</sub>',\n",
        "                'x': 0.5,\n",
        "                'xanchor': 'center',\n",
        "                'font': {'size': 18, 'color': 'rgb(44, 62, 80)'}\n",
        "            },\n",
        "            showlegend=False,\n",
        "            hovermode='closest',\n",
        "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "            plot_bgcolor='white',\n",
        "            width=1400,\n",
        "            height=1000,\n",
        "            margin=dict(l=20, r=20, t=100, b=20)\n",
        "        )\n",
        "\n",
        "        fig.write_html(output_path)\n",
        "        print(f\"‚úì Interactive workflow visualization saved to {output_path}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def visualize_variants(self, top_n=10, output_path='variants_analysis.html'):\n",
        "        \"\"\"Create interactive variant analysis visualization.\"\"\"\n",
        "        if self.variants is None:\n",
        "            variant_stats = self.analyze_variants()\n",
        "        else:\n",
        "            variant_stats = self.analyze_variants()\n",
        "\n",
        "        # Prepare data for top N variants\n",
        "        top_variants = variant_stats.head(top_n)\n",
        "\n",
        "        # Create subplots\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                'Top Process Variants by Frequency',\n",
        "                'Variant Duration Distribution',\n",
        "                'Variant Frequency vs Duration',\n",
        "                'Cumulative Frequency'\n",
        "            ),\n",
        "            specs=[[{\"type\": \"bar\"}, {\"type\": \"box\"}],\n",
        "                   [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]]\n",
        "        )\n",
        "\n",
        "        # 1. Frequency bar chart\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=[f\"V{i+1}\" for i in range(len(top_variants))],\n",
        "                y=top_variants['frequency'],\n",
        "                text=top_variants['percentage'].apply(lambda x: f\"{x}%\"),\n",
        "                textposition='auto',\n",
        "                hovertext=[f\"{v}<br>Frequency: {f} ({p}%)\"\n",
        "                          for v, f, p in zip(top_variants['variant'],\n",
        "                                            top_variants['frequency'],\n",
        "                                            top_variants['percentage'])],\n",
        "                hoverinfo='text',\n",
        "                marker_color='lightblue'\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # 2. Duration box plot\n",
        "        variant_durations = []\n",
        "        variant_labels = []\n",
        "        for i, (_, row) in enumerate(top_variants.iterrows()):\n",
        "            durations = self.variants[self.variants['variant'] == row['variant']]['duration'].values\n",
        "            variant_durations.extend(durations)\n",
        "            variant_labels.extend([f\"V{i+1}\"] * len(durations))\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Box(\n",
        "                y=variant_durations,\n",
        "                x=variant_labels,\n",
        "                marker_color='lightgreen'\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        # 3. Frequency vs Duration scatter\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=top_variants['frequency'],\n",
        "                y=top_variants['avg_duration'],\n",
        "                mode='markers+text',\n",
        "                text=[f\"V{i+1}\" for i in range(len(top_variants))],\n",
        "                textposition='top center',\n",
        "                marker=dict(size=12, color='coral'),\n",
        "                hovertext=[f\"{v}<br>Freq: {f}<br>Avg Duration: {self.format_duration(d)}\"\n",
        "                          for v, f, d in zip(top_variants['variant'],\n",
        "                                            top_variants['frequency'],\n",
        "                                            top_variants['avg_duration'])],\n",
        "                hoverinfo='text'\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # 4. Cumulative frequency\n",
        "        cumsum = top_variants['percentage'].cumsum()\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=[f\"V{i+1}\" for i in range(len(top_variants))],\n",
        "                y=cumsum,\n",
        "                mode='lines+markers',\n",
        "                marker=dict(size=8, color='purple'),\n",
        "                line=dict(width=2),\n",
        "                fill='tonexty',\n",
        "                hovertext=[f\"Cumulative: {c:.1f}%\" for c in cumsum],\n",
        "                hoverinfo='text'\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_xaxes(title_text=\"Variant\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
        "\n",
        "        fig.update_xaxes(title_text=\"Variant\", row=1, col=2)\n",
        "        fig.update_yaxes(title_text=\"Duration (seconds)\", row=1, col=2)\n",
        "\n",
        "        fig.update_xaxes(title_text=\"Frequency\", row=2, col=1)\n",
        "        fig.update_yaxes(title_text=\"Avg Duration (seconds)\", row=2, col=1)\n",
        "\n",
        "        fig.update_xaxes(title_text=\"Variant\", row=2, col=2)\n",
        "        fig.update_yaxes(title_text=\"Cumulative %\", row=2, col=2)\n",
        "\n",
        "        fig.update_layout(\n",
        "            title_text=f\"Process Variant Analysis (Top {top_n})\",\n",
        "            showlegend=False,\n",
        "            height=900,\n",
        "            width=1400\n",
        "        )\n",
        "\n",
        "        fig.write_html(output_path)\n",
        "        print(f\"‚úì Variant analysis visualization saved to {output_path}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def visualize_bottlenecks(self, output_path='bottlenecks_analysis.html'):\n",
        "        \"\"\"Create interactive bottleneck analysis visualization.\"\"\"\n",
        "        if self.bottlenecks is None:\n",
        "            self.detect_bottlenecks()\n",
        "\n",
        "        # Create figure\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=1,\n",
        "            subplot_titles=(\n",
        "                'Bottleneck Transitions (Sorted by Average Duration)',\n",
        "                'Duration Variability Analysis'\n",
        "            ),\n",
        "            row_heights=[0.6, 0.4]\n",
        "        )\n",
        "\n",
        "        # 1. Bottleneck bar chart\n",
        "        bottlenecks_sorted = self.bottlenecks.sort_values('avg_duration', ascending=True)\n",
        "        transition_labels = [f\"{s} ‚Üí {t}\" for s, t in zip(bottlenecks_sorted['source'],\n",
        "                                                           bottlenecks_sorted['target'])]\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                y=transition_labels,\n",
        "                x=bottlenecks_sorted['avg_duration'],\n",
        "                orientation='h',\n",
        "                marker=dict(\n",
        "                    color=bottlenecks_sorted['avg_duration'],\n",
        "                    colorscale='Reds',\n",
        "                    showscale=True,\n",
        "                    colorbar=dict(title=\"Duration (s)\")\n",
        "                ),\n",
        "                text=[self.format_duration(d) for d in bottlenecks_sorted['avg_duration']],\n",
        "                textposition='auto',\n",
        "                hovertext=[f\"{s} ‚Üí {t}<br>\" +\n",
        "                          f\"Avg: {self.format_duration(avg)}<br>\" +\n",
        "                          f\"Median: {self.format_duration(med)}<br>\" +\n",
        "                          f\"Min: {self.format_duration(min_d)}<br>\" +\n",
        "                          f\"Max: {self.format_duration(max_d)}<br>\" +\n",
        "                          f\"Count: {cnt}\"\n",
        "                          for s, t, avg, med, min_d, max_d, cnt in\n",
        "                          zip(bottlenecks_sorted['source'], bottlenecks_sorted['target'],\n",
        "                              bottlenecks_sorted['avg_duration'], bottlenecks_sorted['median_duration'],\n",
        "                              bottlenecks_sorted['min_duration'], bottlenecks_sorted['max_duration'],\n",
        "                              bottlenecks_sorted['count'])],\n",
        "                hoverinfo='text'\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # 2. Variability scatter plot\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=bottlenecks_sorted['avg_duration'],\n",
        "                y=bottlenecks_sorted['cv'],\n",
        "                mode='markers+text',\n",
        "                text=[f\"{s[:10]}‚Üí{t[:10]}\" for s, t in zip(bottlenecks_sorted['source'],\n",
        "                                                             bottlenecks_sorted['target'])],\n",
        "                textposition='top center',\n",
        "                marker=dict(\n",
        "                    size=bottlenecks_sorted['count'] / bottlenecks_sorted['count'].max() * 30 + 10,\n",
        "                    color='coral',\n",
        "                    line=dict(width=1, color='darkred')\n",
        "                ),\n",
        "                hovertext=[f\"{s} ‚Üí {t}<br>\" +\n",
        "                          f\"Avg Duration: {self.format_duration(avg)}<br>\" +\n",
        "                          f\"Std Dev: {self.format_duration(std)}<br>\" +\n",
        "                          f\"CV: {cv:.2f}<br>\" +\n",
        "                          f\"Count: {cnt}\"\n",
        "                          for s, t, avg, std, cv, cnt in\n",
        "                          zip(bottlenecks_sorted['source'], bottlenecks_sorted['target'],\n",
        "                              bottlenecks_sorted['avg_duration'], bottlenecks_sorted['std_duration'],\n",
        "                              bottlenecks_sorted['cv'], bottlenecks_sorted['count'])],\n",
        "                hoverinfo='text'\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_xaxes(title_text=\"Average Duration (seconds)\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"Transition\", row=1, col=1)\n",
        "\n",
        "        fig.update_xaxes(title_text=\"Average Duration (seconds)\", row=2, col=1)\n",
        "        fig.update_yaxes(title_text=\"Coefficient of Variation (CV)\", row=2, col=1)\n",
        "\n",
        "        fig.update_layout(\n",
        "            title_text=\"Bottleneck Analysis<br><sub>Higher CV indicates more variable transition times</sub>\",\n",
        "            showlegend=False,\n",
        "            height=1000,\n",
        "            width=1400\n",
        "        )\n",
        "\n",
        "        fig.write_html(output_path)\n",
        "        print(f\"‚úì Bottleneck analysis visualization saved to {output_path}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def print_kpi_report(self):\n",
        "        \"\"\"Print a formatted KPI report.\"\"\"\n",
        "        if self.kpis is None:\n",
        "            self.compute_kpis()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìä PROCESS MINING KPI REPORT\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"\\nüìÅ Total Cases: {self.kpis['total_cases']}\")\n",
        "        print(f\"üìã Total Events: {self.kpis['total_events']}\")\n",
        "        print(f\"üìà Avg Activities per Case: {self.kpis['avg_activities_per_case']:.1f}\")\n",
        "        print(f\"üìâ Median Activities per Case: {self.kpis['median_activities_per_case']:.0f}\")\n",
        "        print(f\"\\n‚è±Ô∏è  Case Duration Statistics:\")\n",
        "        print(f\"   ‚Ä¢ Average:  {self.format_duration(self.kpis['avg_duration_seconds'])}\")\n",
        "        print(f\"   ‚Ä¢ Median:   {self.format_duration(self.kpis['median_duration_seconds'])}\")\n",
        "        print(f\"   ‚Ä¢ Std Dev:  {self.format_duration(self.kpis['std_duration_seconds'])}\")\n",
        "        print(f\"   ‚Ä¢ Minimum:  {self.format_duration(self.kpis['min_duration_seconds'])}\")\n",
        "        print(f\"   ‚Ä¢ Maximum:  {self.format_duration(self.kpis['max_duration_seconds'])}\")\n",
        "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "    def print_transition_summary(self, top_n=10):\n",
        "        \"\"\"Print summary of most frequent transitions.\"\"\"\n",
        "        metrics = self.compute_transition_metrics()\n",
        "        metrics_sorted = metrics.sort_values('count', ascending=False)\n",
        "\n",
        "        print(f\"\\nüîÑ Top {top_n} Most Frequent Transitions:\")\n",
        "        print(\"-\" * 90)\n",
        "        print(f\"{'Source':<25} {'Target':<25} {'Avg Duration':<15} {'Count':<10}\")\n",
        "        print(\"-\" * 90)\n",
        "\n",
        "        for _, row in metrics_sorted.head(top_n).iterrows():\n",
        "            duration_str = self.format_duration(row['avg_duration'])\n",
        "            print(f\"{row['source']:<25} {row['target']:<25} {duration_str:<15} {int(row['count']):<10}\")\n",
        "\n",
        "        print(\"-\" * 90 + \"\\n\")\n",
        "\n",
        "    def print_variant_summary(self, top_n=5):\n",
        "        \"\"\"Print summary of process variants.\"\"\"\n",
        "        variant_stats = self.analyze_variants()\n",
        "\n",
        "        print(f\"\\nüîÄ Top {top_n} Process Variants:\")\n",
        "        print(\"=\"*100)\n",
        "\n",
        "        for i, (_, row) in enumerate(variant_stats.head(top_n).iterrows(), 1):\n",
        "            print(f\"\\nVariant {i} ({row['percentage']:.1f}% of cases, frequency: {int(row['frequency'])})\")\n",
        "            print(f\"   Path: {row['variant']}\")\n",
        "            print(f\"   Avg Duration: {self.format_duration(row['avg_duration'])}\")\n",
        "            print(f\"   Activities: {int(row['num_activities'])}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
        "\n",
        "    def print_bottleneck_summary(self, top_n=5):\n",
        "        \"\"\"Print summary of detected bottlenecks.\"\"\"\n",
        "        if self.bottlenecks is None:\n",
        "            self.detect_bottlenecks()\n",
        "\n",
        "        print(f\"\\nüö® Top {top_n} Bottlenecks (Slowest Transitions):\")\n",
        "        print(\"-\" * 100)\n",
        "        print(f\"{'Transition':<40} {'Avg Duration':<15} {'Median':<15} {'Count':<10} {'CV':<10}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        for _, row in self.bottlenecks.head(top_n).iterrows():\n",
        "            transition = f\"{row['source'][:15]} ‚Üí {row['target'][:15]}\"\n",
        "            avg_str = self.format_duration(row['avg_duration'])\n",
        "            med_str = self.format_duration(row['median_duration'])\n",
        "            cv_str = f\"{row['cv']:.2f}\"\n",
        "\n",
        "            print(f\"{transition:<40} {avg_str:<15} {med_str:<15} {int(row['count']):<10} {cv_str:<10}\")\n",
        "\n",
        "        print(\"-\" * 100)\n",
        "        print(\"Note: CV (Coefficient of Variation) indicates duration variability. Higher = more variable.\")\n",
        "        print(\"-\" * 100 + \"\\n\")\n",
        "\n",
        "    def run_complete_analysis(self,\n",
        "                            static_output='agent_workflow_static.png',\n",
        "                            interactive_output='agent_workflow_interactive.html',\n",
        "                            variants_output='variants_analysis.html',\n",
        "                            bottlenecks_output='bottlenecks_analysis.html'):\n",
        "        \"\"\"Run the complete process mining analysis pipeline.\"\"\"\n",
        "        print(\"\\nüöÄ Starting Comprehensive Process Mining Analysis...\\n\")\n",
        "\n",
        "        # Compute KPIs\n",
        "        self.compute_kpis()\n",
        "        self.print_kpi_report()\n",
        "\n",
        "        # Extract and summarize transitions\n",
        "        self.extract_transitions()\n",
        "        self.print_transition_summary()\n",
        "\n",
        "        # Analyze variants\n",
        "        self.print_variant_summary()\n",
        "\n",
        "        # Detect bottlenecks\n",
        "        self.detect_bottlenecks()\n",
        "        self.print_bottleneck_summary()\n",
        "\n",
        "        # Generate visualizations\n",
        "        print(\"üìà Generating visualizations...\\n\")\n",
        "\n",
        "        print(\"1Ô∏è‚É£ Creating static workflow diagram...\")\n",
        "        self.visualize_workflow_static(output_path=static_output)\n",
        "\n",
        "        print(\"\\n2Ô∏è‚É£ Creating interactive workflow diagram...\")\n",
        "        self.visualize_workflow_interactive(output_path=interactive_output)\n",
        "\n",
        "        print(\"\\n3Ô∏è‚É£ Creating variant analysis...\")\n",
        "        self.visualize_variants(output_path=variants_output)\n",
        "\n",
        "        print(\"\\n4Ô∏è‚É£ Creating bottleneck analysis...\")\n",
        "        self.visualize_bottlenecks(output_path=bottlenecks_output)\n",
        "\n",
        "        print(\"\\n‚úÖ Complete Analysis Finished!\")\n",
        "        print(f\"\\nüìÇ Generated Files:\")\n",
        "        print(f\"   ‚Ä¢ {static_output}\")\n",
        "        print(f\"   ‚Ä¢ {interactive_output}\")\n",
        "        print(f\"   ‚Ä¢ {variants_output}\")\n",
        "        print(f\"   ‚Ä¢ {bottlenecks_output}\")\n",
        "\n",
        "        return self\n"
      ],
      "metadata": {
        "id": "l-g_7jzjUsyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_logs"
      ],
      "metadata": {
        "id": "f5S9H_3DrMOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# USAGE EXAMPLE\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    all_logs = []\n",
        "    # Path to file with all prompts\n",
        "    file_path = 'miningAgent/docs/agent_log.csv'\n",
        "\n",
        "    # Read logs and transform the data\n",
        "    with open(file_path, mode='r', newline='', encoding='utf-8') as csv_file:\n",
        "        # Use csv.DictReader to map the header row to keys\n",
        "        csv_reader = csv.DictReader(csv_file)\n",
        "        # Iterate over all subsequent rows\n",
        "        for row in csv_reader:\n",
        "            # Check if the action is 'Decision Finalized'\n",
        "            if row.get('action') == 'Decision Finalized':\n",
        "                comment = row.get('comment', '')\n",
        "\n",
        "                # This captures any character ([^,]+) after 'decision:' up to a comma,\n",
        "                # correctly handling multi-word decisions like 'Not Covered'.\n",
        "                match = re.search(r'decision:\\s*([^,]+)', comment)\n",
        "\n",
        "                if match:\n",
        "                    # Extract the captured group (the decision value) and strip whitespace\n",
        "                    decision = match.group(1).strip()\n",
        "                    # Replace the 'action' field with the final decision\n",
        "                    row['action'] = decision\n",
        "\n",
        "            # Append the (potentially modified) row to the list\n",
        "            all_logs.append(row)\n",
        "\n",
        "    try:\n",
        "        tool = ProcessMiningTool(all_logs)\n",
        "\n",
        "        # Run complete analysis\n",
        "        tool.run_complete_analysis(\n",
        "            static_output='agent_workflow_static.png',\n",
        "            interactive_output='agent_workflow_interactive.html',\n",
        "            variants_output='variants_analysis.html',\n",
        "            bottlenecks_output='bottlenecks_analysis.html'\n",
        "        )\n",
        "\n",
        "        # Optional: Access individual components\n",
        "        graph = tool.build_process_graph()\n",
        "        kpis = tool.compute_kpis()\n",
        "        variants = tool.analyze_variants()\n",
        "        bottlenecks = tool.detect_bottlenecks()\n",
        "        transitions = tool.extract_transitions()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: File '{file_path}' not found!\")\n",
        "        print(\"Trying with Sample.csv instead...\")\n",
        "\n",
        "        # Example 2: Direct CSV file path (fallback)\n",
        "        try:\n",
        "            tool = ProcessMiningTool('Sample.csv')\n",
        "            tool.run_complete_analysis()\n",
        "        except FileNotFoundError:\n",
        "            print(\"‚ùå Error: Sample.csv also not found!\")\n",
        "            print(\"Please ensure a CSV file is available.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error occurred: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "kF6WofzmU3k1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}